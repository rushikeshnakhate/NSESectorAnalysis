{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushikeshnakhate/NSESectorAnalysis/blob/main/nsesectorsanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq_L1P3cUsSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439c851b-c982-4647-f00c-652bd9c8509f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jugaad_data -q\n",
        "!pip install dataclass -q"
      ],
      "metadata": {
        "id": "eDgwA2d4U03A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1083b2-4108-46ea-e4ee-e7a71e850336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.8/115.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.6 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\n",
            "yfinance 0.2.38 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.9.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement dataclass (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for dataclass\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import date\n",
        "from jugaad_data.nse import NSELive\n",
        "from jugaad_data.nse import bhavcopy_save\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from datetime import date, timedelta\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "output_dir = Path(\"/content/drive/MyDrive\")\n",
        "sectorFile = output_dir.joinpath(\"Sector.csv\")\n",
        "holidayFile = output_dir.joinpath(\"Holiday.csv\")\n",
        "correlationFile= output_dir.joinpath(\"Correlation.csv\")\n"
      ],
      "metadata": {
        "id": "QUIGKXhdVMTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_required_args(kwargs, *required_args):\n",
        "    for arg in required_args:\n",
        "        if arg not in kwargs:\n",
        "            raise ValueError(f\"Error: '{arg}' argument not provided, actual received: {kwargs}\")\n",
        "\n",
        "def load_or_download_and_cache_data(download_function:callable,**kwargs):\n",
        "  validate_required_args(kwargs, 'cache_file')\n",
        "  cache_file : str = kwargs.get(\"cache_file\")\n",
        "\n",
        "  if os.path.exists(cache_file):\n",
        "    with open(cache_file,'rb') as file:\n",
        "      return pickle.load(file)\n",
        "\n",
        "  data_df = download_function(**kwargs)\n",
        "  # if not data_df.empty:\n",
        "  with open(cache_file,'wb') as file:\n",
        "    pickle.dump(data_df,file)\n",
        "  return data_df\n"
      ],
      "metadata": {
        "id": "DHDyt2tEVQSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_bhavcopy(**kwargs):\n",
        "  \"\"\"\n",
        "  # bhavcopy_save: Generates a CSV file with a random name.\n",
        "  # The expected name for the downloaded Bhavcopy file is \"bhavcopy_{start_date}.csv\". # The cache file is named as \"bhavcopy_\n",
        "{start_date}.pkl\". A renaming process is required.\n",
        "  \"\"\"\n",
        "  validate_required_args(kwargs, 'cache_file','start_date')\n",
        "  cache_file: str = kwargs.get(\"cache_file\")\n",
        "  start_date : date= kwargs.get(\"start_date\")\n",
        "\n",
        "  generated_csv_filename = bhavcopy_save(start_date, output_dir)\n",
        "  cache_file_to_csv_name = cache_file.with_suffix(\".csv\")\n",
        "  os.rename(generated_csv_filename,cache_file_to_csv_name)\n",
        "  return pd.read_csv(cache_file_to_csv_name)\n"
      ],
      "metadata": {
        "id": "PuBXUPvpVSCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_or_genenate_stocks(start_date: date):\n",
        "  output_file  = Path(output_dir)/(\"NseData_\" + str(start_date) +\".pkl\")\n",
        "  return load_or_download_and_cache_data(download_bhavcopy,start_date= start_date,cache_file=output_file)\n",
        "\n",
        "def get_sectors():\n",
        "    sector_df = pd.read_csv(sectorFile)\n",
        "    return sector_df"
      ],
      "metadata": {
        "id": "79wuShPxVTHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_or_generate_sector_to_stocks(stocks : pd.DataFrame):\n",
        "  output_file  = Path(output_dir)/(\"Sectors.pkl\")\n",
        "  sector_df = get_sectors()\n",
        "  # display(stocks_to_sector_map_df)\n",
        "  try:\n",
        "   return pd.merge(stocks, sector_df, on='SYMBOL', how='inner',suffixes=('', ''))\n",
        "  except pd.errors.MergeError as e:\n",
        "    print(f\"Merge failed: {e}\")"
      ],
      "metadata": {
        "id": "6fq8w54ZVUJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_performace(stocks_df : pd.DataFrame, start_date: date):\n",
        "    # Calculate daily returns\n",
        "  print(\"calculate_performace for date={}\".format(start_date))\n",
        "\n",
        "\n",
        "  stocks_df['DAILY_RETURN'] =(stocks_df['CLOSE'] - stocks_df['OPEN'])/ stocks_df['OPEN']\n",
        "  # Group by 'SECTOR' and calculate average daily return and volatility\n",
        "  sector_performance_df = stocks_df.groupby('SECTOR').agg({\n",
        "      # stocks_list=('STOCK', lambda x: list(x))\n",
        "      'DAILY_RETURN': ['std','mean','count']\n",
        "  }).reset_index()\n",
        "\n",
        "  sector_performance_df[\"TRADEDATE\"] = start_date\n",
        "\n",
        "  # Flatten the column names, but only for columns where the second part is not an empty string\n",
        "  sector_performance_df.columns = [\n",
        "      f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in sector_performance_df.columns\n",
        "  ]\n",
        "  return sector_performance_df\n",
        "\n",
        "\n",
        "def prepare_stocks_data_and_calculate_performance(**kwargs):\n",
        "  try:\n",
        "    validate_required_args(kwargs, 'start_date')\n",
        "    start_date : date = kwargs.get('start_date')\n",
        "    stocks_df = fetch_or_genenate_stocks(start_date)\n",
        "    stocks_df_with_sector = fetch_or_generate_sector_to_stocks(stocks_df)\n",
        "    return calculate_performace(stocks_df_with_sector,start_date)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "\n",
        "def fetch_or_generate_performance_for_stocks(start_date:date):\n",
        "  sector_performance_output_file = Path(output_dir)/(\"SectorPeformance_\" + str(start_date) +\".pkl\")\n",
        "  return load_or_download_and_cache_data(prepare_stocks_data_and_calculate_performance,start_date=start_date,cache_file=sector_performance_output_file)\n"
      ],
      "metadata": {
        "id": "k3RIUocnVVKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_weekend(date : date):\n",
        "    day = date.weekday()\n",
        "    if day == 5 or day == 6:\n",
        "        return True\n",
        "\n",
        "def is_public_holiday(date : date):\n",
        "    public_holidays_df = pd.read_csv(holidayFile)\n",
        "    return date.strftime('%d-%m-%Y') in public_holidays_df['Date'].values\n"
      ],
      "metadata": {
        "id": "L-ZyFXasVWKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uniq_sectors():\n",
        "  sectors_csv = pd.read_csv(sectorFile)\n",
        "  sectors = sectors_csv['SECTOR'].unique()\n",
        "  sectors_stripped = [sector.strip() for sector in sectors]\n",
        "  unique_sectors = list(set(sectors_stripped))\n",
        "  return unique_sectors\n"
      ],
      "metadata": {
        "id": "AnbeYeyC4g7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CorrelationData:\n",
        "    #Date: date\n",
        "    FirstSector: str\n",
        "    SecondSector: str\n",
        "    CorrelationByMean: float\n",
        "    CorrelationByDeviation: float\n",
        "\n",
        "def populate_sectors_correlation_df(generate_performance_for_stocks_group_by_sector):\n",
        "    correlation_data_list = []\n",
        "    sector_list_mean_matrix = {}\n",
        "    sector_list_dev_matrix = {}\n",
        "    dataframe_correlation = pd.DataFrame()\n",
        "\n",
        "    sectors = get_uniq_sectors()\n",
        "    for sector in sectors:\n",
        "        sector_list_mean_matrix[sector] = []\n",
        "        sector_list_dev_matrix[sector] = []\n",
        "\n",
        "    if not generate_performance_for_stocks_group_by_sector.empty and not generate_performance_for_stocks_group_by_sector.empty:\n",
        "      for sector in sectors:\n",
        "        for index, row in generate_performance_for_stocks_group_by_sector.iterrows():\n",
        "          if row['SECTOR']  ==  sector:\n",
        "             sector_list_mean_matrix[sector].append(row['DAILY_RETURN_mean'])\n",
        "             sector_list_dev_matrix[sector].append(row['DAILY_RETURN_std'])\n",
        "    dataframe_correlation = pd.DataFrame(sector_list_mean_matrix)\n",
        "\n",
        "\n",
        "    for i in range(len(sectors)):\n",
        "         for j in range(i + 1, len(sectors)):\n",
        "            first_sector = sectors[i]\n",
        "            second_sector = sectors[j]\n",
        "            #print(first_sector,second_sector )\n",
        "            correlation_mean = pd.Series(sector_list_mean_matrix[first_sector]).corr(pd.Series(sector_list_mean_matrix[second_sector]))\n",
        "            correlation_std = pd.Series(sector_list_dev_matrix[first_sector]).corr(pd.Series(sector_list_dev_matrix[second_sector]))\n",
        "            correlation_data = CorrelationData(FirstSector=first_sector, SecondSector=second_sector, CorrelationByMean=correlation_mean, CorrelationByDeviation=correlation_std)\n",
        "            correlation_data_list.append(correlation_data)\n",
        "\n",
        "    correlation_df = pd.DataFrame([vars(x) for x in correlation_data_list])\n",
        "    return  correlation_df\n",
        "\n",
        "# Better version of above populate_sectors_correlation_df\n",
        "def populate_sectors_correlation_df_1(generate_performance_for_stocks_group_by_sector , correlation_method= \"pearson\"):\n",
        "\n",
        "  if correlation_method not in [\"pearson\", \"spearman\", \"kendall\"]:\n",
        "    raise RuntimeError(\"Invalid correlation method. Supported methods are 'Pearson', 'Spearman', and 'Kendall'.\")\n",
        "\n",
        "\n",
        "  correlation_data_list = []\n",
        "  sector_list_mean_matrix = {}\n",
        "  sector_list_dev_matrix = {}\n",
        "  sectors = get_uniq_sectors()\n",
        "\n",
        "  for sector in sectors:\n",
        "      sector_list_mean_matrix[sector] = []\n",
        "      sector_list_dev_matrix[sector] = []\n",
        "\n",
        "  if not generate_performance_for_stocks_group_by_sector.empty and not generate_performance_for_stocks_group_by_sector.empty:\n",
        "      for sector in sectors:\n",
        "          sector_data = generate_performance_for_stocks_group_by_sector[generate_performance_for_stocks_group_by_sector['SECTOR'] == sector]\n",
        "          sector_list_mean_matrix[sector] = sector_data['DAILY_RETURN_mean'].tolist()\n",
        "          sector_list_dev_matrix[sector] = sector_data['DAILY_RETURN_std'].tolist()\n",
        "\n",
        "  for i in range(len(sectors)):\n",
        "      for j in range(i + 1, len(sectors)):\n",
        "          first_sector = sectors[i]\n",
        "          second_sector = sectors[j]\n",
        "\n",
        "          if correlation_method == \"pearson\":\n",
        "            correlation_mean = pd.Series(sector_list_mean_matrix[first_sector]).corr(pd.Series(sector_list_mean_matrix[second_sector]))\n",
        "            correlation_std = pd.Series(sector_list_dev_matrix[first_sector]).corr(pd.Series(sector_list_dev_matrix[second_sector]))\n",
        "          elif correlation_method == \"spearman\":\n",
        "            correlation_mean = pd.Series(sector_list_mean_matrix[first_sector]).corr(pd.Series(sector_list_mean_matrix[second_sector]),method='spearman')\n",
        "            correlation_std = pd.Series(sector_list_dev_matrix[first_sector]).corr(pd.Series(sector_list_dev_matrix[second_sector]),method='spearman')\n",
        "          elif correlation_method == \"kendall\":\n",
        "            correlation_mean = pd.Series(sector_list_mean_matrix[first_sector]).corr(pd.Series(sector_list_mean_matrix[second_sector]),method='kendall')\n",
        "            correlation_std = pd.Series(sector_list_dev_matrix[first_sector]).corr(pd.Series(sector_list_dev_matrix[second_sector]),method='kendall')\n",
        "          # correlation_mean = pd.Series(sector_list_mean_matrix[first_sector]).corr(pd.Series(sector_list_mean_matrix[second_sector]))\n",
        "          # correlation_std = pd.Series(sector_list_dev_matrix[first_sector]).corr(pd.Series(sector_list_dev_matrix[second_sector]))\n",
        "          correlation_data_list.append({'FirstSector': first_sector, 'SecondSector': second_sector, 'CorrelationByMean': correlation_mean, 'CorrelationByDeviation': correlation_std})\n",
        "\n",
        "  correlation_df = pd.DataFrame(correlation_data_list)\n",
        "  return correlation_df\n"
      ],
      "metadata": {
        "id": "8Ppy5fhBuVMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date, timedelta\n",
        "import calendar\n",
        "\n",
        "def generate_dates(year, start_month, end_month):\n",
        "    start_date = date(year, start_month, 1)\n",
        "    # Find the last day of the end month\n",
        "    _, last_day = calendar.monthrange(year, end_month)\n",
        "    end_date = date(year, end_month, last_day)\n",
        "\n",
        "    current_date = start_date\n",
        "    extended_dates = []\n",
        "    while current_date <= end_date:\n",
        "        extended_dates.append(current_date)\n",
        "        current_date += timedelta(days=1)\n",
        "    return extended_dates\n",
        "\n",
        "\n",
        "\n",
        "def plot (correlation_df):\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  sns.heatmap(correlation_df.reindex(index=sectors, columns=sectors), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "  plt.title('Correlation Heatmap between Sectors')\n",
        "  plt.xlabel('SecondSector')\n",
        "  plt.ylabel('FirstSector')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def  get_corelation_of_one_sector_with_all_by(column,correlation_df):\n",
        "  correlation_matrix = {}\n",
        "  for index, row in correlation_df.iterrows():\n",
        "    if row['FirstSector'] not in correlation_matrix:\n",
        "        correlation_matrix[row['FirstSector']] = {}\n",
        "    correlation_matrix[row['FirstSector']][row['SecondSector']] = row[column]\n",
        "\n",
        "    if row['SecondSector'] not in correlation_matrix:\n",
        "        correlation_matrix[row['SecondSector']] = {}\n",
        "    correlation_matrix[row['SecondSector']][row['FirstSector']] = row[column]\n",
        "\n",
        "  for sector in correlation_matrix.keys():\n",
        "    correlation_matrix[sector][sector] = 1\n",
        "\n",
        "  correlation_df = pd.DataFrame(correlation_matrix)\n",
        "  return correlation_df\n"
      ],
      "metadata": {
        "id": "k0K4dVfwExkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "if __name__ == \"__main__\":\n",
        "  startdates = []\n",
        "  for year in range(2023, 2013, -1):  # Loop from 2023 to 2013 (inclusive)\n",
        "    startdates.extend(generate_dates(year, 1, 12))\n",
        "\n",
        "  startdates = list(set(startdates))\n",
        "  dfs = []\n",
        "  for startdate in startdates:\n",
        "    if is_weekend(startdate) or is_public_holiday(startdate):\n",
        "        continue\n",
        "    try:\n",
        "        generate_performance_for_stocks_group_by_sector = fetch_or_generate_performance_for_stocks(startdate)\n",
        "        stocks = fetch_or_genenate_stocks(startdate)\n",
        "        stocks_wth_sectors = fetch_or_generate_sector_to_stocks(stocks)\n",
        "        dfs.append(generate_performance_for_stocks_group_by_sector)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate performance for {startdate}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "  correlationMethods = [\"pearson\",\"spearman\",\"kendall\"]\n",
        "  for correlationMethod in correlationMethods:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    correlation_df = populate_sectors_correlation_df_1(combined_df,correlationMethod)\n",
        "    correlation_matrix_by_mean = get_corelation_of_one_sector_with_all_by('CorrelationByMean',correlation_df)\n",
        "    correlation_matrix_deviation = get_corelation_of_one_sector_with_all_by('CorrelationByDeviation',correlation_df)\n",
        "    display(correlation_matrix_by_mean)\n",
        "    display(correlation_matrix_deviation)\n"
      ],
      "metadata": {
        "id": "S8W5hd44jDMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd20654-8654-4a8f-95ea-2fc3eef73daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate performance for 2021-11-19: File is not a zip file\n",
            "Failed to generate performance for 2019-08-12: File is not a zip file\n",
            "Failed to generate performance for 2017-08-25: File is not a zip file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NSE Index Performance:\n",
        "# Trend Observation: Look at the overall trend of the NSE Index line. An upward trend suggests a positive week for the market, whereas a downward trend indicates a bearish market. Stability or minor fluctuations might suggest a period of consolidation.\n",
        "# Conclusion: If the index shows a significant upward trend, you could conclude that the market was optimistic during that week. Conversely, a downward trend could indicate market pessimism or reaction to adverse events.\n",
        "# Sector Mean Returns:\n",
        "# Performance Comparison: Identify which sectors had the highest mean returns. High positive mean returns suggest sectors that outperformed during the week.\n",
        "# Conclusion: Sectors with the highest mean returns were the leading performers and potentially received positive news or investor sentiment. Sectors with negative returns might have faced challenges or negative sentiment.\n",
        "# Sector Volatility:\n",
        "# Risk Assessment: Volatility indicates risk. Higher volatility suggests greater uncertainty or variability in returns.\n",
        "# Conclusion: Sectors with high volatility experienced more significant price movements and could be considered riskier investments during that week. Lower volatility suggests stability, potentially making them safer choices for risk-averse investors.\n",
        "# Sector Weighted Average Returns:\n",
        "# Impact of Key Stocks: This metric shows the overall sector performance considering the size or importance of stocks (e.g., by trading volume).\n",
        "# Conclusion: If certain sectors show high weighted average returns, it indicates that larger, possibly more influential stocks performed well, driving the sector's overall performance. Comparing weighted average returns to mean returns can reveal the impact of larger stocks on sector performance.\n",
        "# General Conclusions:\n",
        "# Market Overview: The combined view of the NSE Index and sector performance metrics offers a comprehensive market overview, highlighting overall market trends, sector-specific performances, and risk profiles.\n",
        "# Sector Analysis: By examining all metrics together, you can identify sectors that are not only performing well but are also less volatile, suggesting a favorable risk-return profile. Conversely, sectors with poor or volatile performance might warrant caution or further investigation.\n",
        "# Investment Insights: Investors might find opportunities in sectors with strong, stable performance. High volatility sectors might offer short-term trading opportunities but come with higher risk.\n",
        "# Limitations:\n",
        "# Short Time Frame: Conclusions drawn from a week's data are inherently limited and should be considered in the context of longer-term trends and external factors (e.g., economic news, global events).\n",
        "# Data Depth: Additional insights could be gained from more detailed data, such as price-to-earnings ratios, dividend yields, or macroeconomic indicators, for a fuller analysis.\n",
        "# These conclusions are based on hypothetical data and general principles of financial analysis. Actual data might lead to different interpretations, and any investment decisions should be made considering comprehensive analysis and personal investment goals."
      ],
      "metadata": {
        "id": "iizpyta9Avgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yPS05KCoh1Dl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}